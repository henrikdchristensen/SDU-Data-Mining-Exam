\section{Introduction}
\textit{Clustering} is one of the main techniques within data mining. The main goal is to discover unknown patterns within a data set, by partitioning the data objects into \textit{clusters}. Here, each object in a cluster is similar to one another, but different from objects in other clusters. Clustering is widely used in many applications, such as advertising, biology, web search and business intelligence \cite[p.~444]{han-2011}.

The task of clustering data is a challenging task, first, the data sets are typical large in size, which means that the clustering algorithm must be \textit{scalable}. Additionally, data sets often contains numerous features (\textit{attributes}), which introduces the problem of \textit{curse of dimensionality}, which refers to a several challenges related to high-dimensional data spaces:

First, the issue of \textit{concentration of distances}, where distances between objects in high-dimensional spaces become increasingly similar as dimensionality increases. This means that data points tend to become nearly equidistant from one another, making it difficult for traditional distance-based algorithms to discover clusters.

Secondly, there is the problem of \textit{local feature relevance} and \textit{local feature correlation}, where only a subset of features or different combinations of feature correlations may be relevant for clustering. Consequently, feature reduction techniques like \textit{Principal Component Analysis} (PCA), which project the original space onto a lower-dimensional subspace, are inadequate because they typically identify only one global subspace that best fits the entire dataset. Also, algorithms that evaluate the entire feature space does not address this issue effectively. \cite[p.~43--46]{kriegel-2009}

Instead of relying on a global approach to feature selection, a local approach that addresses the issues of local feature relevance and local feature correlation is necessary. However, we then need to deal with two separate problems, which both needs to be solved simultaneously. First, is the problem of finding the relevant subspaces of each cluster. Second, is the problem of finding the clusters in each relevant subspace. To solve them efficiently, heuristics needs to be employed into the clustering algorithms. \cite[p.~6--7]{kriegel-2009}

For many applications, it is reasonable to focus only on clusters in axis-parallel subspaces, thus restricting the search space to $O(2^d)$ dimensions. These algorithms are called \textit{subspace clustering} (or \textit{projected clustering}) algorithms. These can be further divided into: \textit{top-down}- or \textit{bottom-up} approach. In top-down approach, the relevant subspaces for the clusters are determined by gradually reducing the subspaces, starting from the entire space. In contrast, bottom-up approaches, finds the relevant subspaces for the clusters from the original space starting from 1-dimensional using the \textit{monotonicity property} (or \textit{downward closure property}), see Lemma \ref{lem:mono} \cite{clique}. \cite[p.~8,~11]{kriegel-2009}

For the rest of this paper, the following notation will be adopted: Let $\mathcal{A} = \{A_1, \dots, A_d\}$ represent the set of dimensions (attributes) in a $d$-dimensional numerical space, $\mathcal{S} = A_1 \times A_2 \times \dots \times A_d$. A subset of $\mathcal{A}$ is called a subspace. Let $\mathcal{D} = \{p_1, \dots, p_n\}$ be the dataset, where $p_i \in \mathcal{S}$ represents a data point. A cluster $\mathcal{C}$ is a subset of $\mathcal{D}$, where each point $p_i \in \mathcal{C}$ is more similar to the other points in $\mathcal{C}$ than to points outside of $\mathcal{C}$.

\begin{lemma}\label{lem:mono}
    If $\mathcal{C}$ is a cluster in a $k$-dimensional subspace, then $\mathcal{C}$ must also form a cluster in any $(k-1)$-dimensional projection of the same space.
\end{lemma}
A proof is provided in \cite{clique}.

\subsection{Contributions}
The primary focus of this paper is to analyze the grid-based bottom-up subspace clustering algorithm \textit{MAFIA} \cite{mafia}, which builds upon the first grid-based approach \textit{CLIQUE} \cite{clique}. This paper examines the relationship between these two, highlighting their similarities and differences. Additionally, the density-connectivity-based algorithm \textit{SUBCLU} \cite{subclu} is included, as it offers a contrasting approach to the grid-based approach.

The remainder of the paper is structured as follows: Section 2 describes and analyzes the three algorithms in detail. Section 3 evaluates their performance in terms of scalability, considering dataset size, data- and cluster-dimensionality, as well as their clustering quality. Section 5 discusses the findings and explores the contributions of each algorithm to the field of subspace clustering. Finally, Section 6 concludes the main findings and suggests some future work.