\section{Introduction}
\textit{Clustering} is one of the main techniques within data mining. The main goal is to discover unknown patterns within a data set, by partitioning the data objects into \textit{clusters} in reasonable amount of time. Here, each object in a cluster is similar to one another, but different from objects in other clusters. \cite[p.~444]{han-2011}.

As the data sets, today, often are large in size, the clustering algorithm must be \textit{scalable}. Additionally, data sets often contains numerous features/dimensions (\textit{attributes}), which introduces the problem of \textit{curse of dimensionality}, which refers to mainly two challenges: (1) \textit{concentration of distances}, where distances between objects in high-dimensional spaces become increasingly similar as dimensionality increases. This means that data points tend to become nearly equidistant from one another, making it difficult for traditional distance-based clustering algorithms to discover clusters. (2) \textit{feature relevance} and \textit{local feature correlation}, where only a subset of features or different combinations of feature correlations may be relevant for clustering. Consequently, feature reduction techniques like \textit{Principal Component Analysis} (PCA), which project the original space onto a lower-dimensional subspace, are inadequate because they typically identify only one global subspace that fits best. Also, algorithms that evaluate the entire feature space does not address this issue effectively. \cite[p.~43--46]{kriegel-2009}

Therefore, a local approach essential to address the second problem. However, this introduces two separate problems that the clustering algorithm must solve simultaneously: (1) identifying the relevant subspaces for each cluster, and (2) discovering the clusters within those relevant subspaces. To solve them efficiently, heuristics needs to be employed into the clustering algorithm. \cite[p.~6--7]{kriegel-2009}

A common heuristic used is to focus on clusters within axis-parallel subspaces, which limits the search space to $O(2^d)$ attributes. Algorithms that follow this strategy are known as \textit{subspace clustering} (or \textit{projected clustering}) algorithms. These algorithms follows one of two approaches: the \textit{top-down} or the \textit{bottom-up} approach. In the top-down approach, the relevant subspaces for the clusters are determined by gradually reducing the subspaces, starting from the entire space. In contrast, the bottom-up approach, finds the relevant subspaces for the clusters from the original space starting from 1-dimensional using the \textit{monotonicity property} (or \textit{downward closure property}), see Lemma \ref{lem:mono} \cite{clique}. \cite[p.~8,~11]{kriegel-2009}

For the remainder of this paper, the following notation will be used: Let $\mathcal{A} = \{A_1, \dots, A_d\}$ represent the set of \textit{attributes} in a $d$-dimensional numerical space, and let $\mathcal{S} = A_1 \times A_2 \times \dots \times A_d$ be the full attribute space. A \textit{subspace} is then defined as any subset of $\mathcal{A}$. Let $\mathcal{D}$ denote the \textit{dataset}, and a \textit{cluster} $\mathcal{C}$ is a subset of $\mathcal{D}$.

\begin{lemma}\label{lem:mono}
    If $\mathcal{C}$ is a cluster in a $k$-dimensional subspace, then $\mathcal{C}$ must also form a cluster in any $(k-1)$-dimensional projection of the same space.
\end{lemma}
A proof is provided in \cite{clique}.

\subsection{Contributions}
This paper focuses on analyzing the grid-based bottom-up subspace clustering algorithm MAFIA \cite{mafia}, which extends the earlier grid-based approach CLIQUE \cite{clique}. The relationship between these two algorithms is closely examined. Additionally, the density-connected algorithm SUBCLU \cite{subclu} is included as a contrasting method to the grid-based approaches. All three algorithms are evaluated experimentally, with a focus on scalability and clustering quality.

The remainder of the paper is organized as follows: Section 2 provides a detailed description and analysis of the three algorithms. Section 3 presents an evaluation of their performance, focusing on scalability with respect to dataset size, data- and cluster-dimensionality, and clustering quality. Section 4 discusses the results and the methodologies used. Finally, Section 5 concludes with the main findings and proposes future work.