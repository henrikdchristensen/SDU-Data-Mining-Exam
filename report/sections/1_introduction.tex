\section{Introduction}
\textit{Clustering} is one of the main techniques within data mining. This technique is a descriptive method that tries to discover unknown patterns within a data set, by partitioning the data objects into subsets (\textit{clusters}). Here, each object in a cluster is similar to one another, but different from objects in other clusters. Clustering is widely used in many applications, such as biology, web search and business intelligence \cite[p.~444]{han-2011}.

A simple clustering example is in the context of customer data, where it is useful to group similar customers together for the purpose of targeted advertising or placement of products within a store \cite[p.~5]{kriegel-2009}.

The task of clustering data is challenging task, first of all as the data sets typical is large in size, which means that the clustering algorithm must be \textit{scalable}. Additionally, data sets often contains numerous features (\textit{attributes}), which introduces the problem of \textit{curse of dimensionality}, which refers to a several challenges related to high-dimensional data spaces:

First, there is the issue of \textit{concentration of distances}, a phenomenon in high-dimensional spaces where the distances between objects become increasingly similar as dimensionality increases. This means that data points tend to become nearly equidistant from one another, making it difficult for traditional distance-based algorithms to discover clusters.

Secondly, there is the problem of \textit{local feature relevance} and \textit{local feature correlation}, where only a subset of features or different combinations of feature correlations may be relevant for clustering. Consequently, feature reduction techniques like Principal Component Analysis (PCA), which project the original space onto a lower-dimensional subspace, are inadequate because they typically identify only one global subspace that best fits the entire dataset. Similarly, clustering algorithms that evaluate the entire feature space, such as DBSCAN, struggle to address this issue effectively. \cite[p.~43--46]{kriegel-2009}

Instead of relying on a global approach to feature selection, a local approach that addresses the issues of local feature relevance and local feature correlation is necessary. However, when clustering high-dimensional data we encounter two separate problems, which, however, both needs to be solved simultaneously. First, is the problem of finding the relevant subspaces of each cluster. Second, is the problem of finding the clusters in each relevant subspace. In the first problem, notice that the search space is in general infinite and for the second problem, to find the best partitioning of the objects is NP-complete. To solve them simultaneously, we need to employ heuristics into the clustering algorithms. \cite[p.~6--7]{kriegel-2009}

For many applications, it is reasonable to focus only on clusters in axis-parallel subspaces, thus restricting the search space to $O(2^d)$ dimensions. These algorithms are often called \textit{projected clustering} or \textit{subspace clustering} algorithms. Furthermore, these can be divided into two categories: \textit{top-down}- and \textit{bottom-up} approaches. In the top-down approach, the relevant subspaces for the clusters are determined starting from the full-space either using the so called \textit{locality assumption} or using a random sampling. In contrast, bottom-up approaches, finds the relevant subspaces for the clusters from the original space starting from one-dimensional using the \textit{downward closure property} (also called monotonicity property), that says, \textit{if a subspace contains a cluster, then a superspace must also contain a cluster}, which can be used to prune (exclude) subspaces. \cite[p.~8,~11]{kriegel-2009}

\subsection{Contributions}
This paper examines three different bottom-up subspace clustering algorithms, with a primary focus on the MAFIA algorithm \cite{mafia}, a grid-based method that partitions the data space into adaptive grids using histograms. Since MAFIA extends the pioneering subspace clustering algorithm called CLIQUE \cite{clique}, a comparative analysis between the two will be conducted. Since the two algorithms both are being grid-based, which may limit clusters to hyper-rectangles, a more flexible SUBCLU algorithm \cite{subclu}, which uses density-connected sets to allow clusters with arbitrary shapes, will also be analyzed and evaluated.

The remainder of the paper is organized as follows. In Section 2, first the two grid-based algorithms CLIQUE and MAFIA are analyzed and how they relate. Additionally, the density-based algorithm approach of SUBCLU will be analyzed and how it differ from the grid-based approach, as well as how it differ from the full-dimensional density-based approach of the well-known DBSCAN algorithm \cite{dbscan}. Section 3 gives a more detailed description of the MAFIA algorithm. Section 4 evaluate the performance of the three algorithms in terms of scalability, considering both data dimensionality and cluster dimensionality, as well as their overall cluster quality. Section 5 discusses the pros and cons of the three algorithms. Finally, Section 6 draws conclusions and points out future work.