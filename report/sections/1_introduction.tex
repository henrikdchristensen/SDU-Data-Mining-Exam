\section{Introduction}
\textit{Clustering} is one of the main techniques within data mining. The main goal is to discover unknown patterns within a data set, by partitioning the data objects into \textit{clusters} in reasonable amount of time. Here, each object in a cluster is similar to one another, but different from objects in other clusters. Clustering is widely used in many applications, such as advertising and biology \cite[p.~444]{han-2011}.

As the data sets, today, often are large in size, the clustering algorithm must be \textit{scalable}. Additionally, data sets often contains numerous features/dimensions (\textit{attributes}), which introduces the problem of \textit{curse of dimensionality}, which refers to a several challenges: First, the issue of \textit{concentration of distances}, where distances between objects in high-dimensional spaces become increasingly similar as dimensionality increases. This means that data points tend to become nearly equidistant from one another, making it difficult for traditional distance-based clustering algorithms to discover clusters. Secondly, the problem of \textit{local feature relevance} and \textit{local feature correlation}, where only a subset of features or different combinations of feature correlations may be relevant for clustering. Consequently, feature reduction techniques like \textit{Principal Component Analysis} (PCA), which project the original space onto a lower-dimensional subspace, are inadequate because they typically identify only one global subspace that best fits the entire dataset. Also, algorithms that evaluate the entire feature space does not address this issue effectively. \cite[p.~43--46]{kriegel-2009}

Instead of relying on a global approach to feature selection, a local approach that addresses the second problem is necessary. However, then two separate problems needs to be dealt with, which both needs to be solved simultaneously. First of all, the clustering algorithm must be able to find the relevant subspaces for each cluster. Secondly, it must be able to find the clusters in each relevant subspace. To solve them efficiently, heuristics needs to be employed into the clustering algorithms. \cite[p.~6--7]{kriegel-2009}

For many applications, it is reasonable to focus only on clusters in axis-parallel subspaces, thus restricting the search space to $O(2^d)$ attributes. These algorithms are called \textit{subspace clustering} (or \textit{projected clustering}) algorithms, which uses one of two approaches: the \textit{top-down} approach or the \textit{bottom-up} approach. In the top-down approach, the relevant subspaces for the clusters are determined by gradually reducing the subspaces, starting from the entire space. In contrast, the bottom-up approach, finds the relevant subspaces for the clusters from the original space starting from 1-dimensional using the \textit{monotonicity property} (or \textit{downward closure property}), see Lemma \ref{lem:mono} \cite{clique}. \cite[p.~8,~11]{kriegel-2009}

For the rest of this paper, the following notation will be adopted: Let $\mathcal{A} = \{A_1, \dots, A_d\}$ represent the set of attributes in a $d$-dimensional numerical space, $\mathcal{S} = A_1 \times A_2 \times \dots \times A_d$, then a subspace is a subset of $\mathcal{A}$. Let $\mathcal{D}$ be the dataset, then a cluster $\mathcal{C}$ is a subset of $\mathcal{D}$.

\begin{lemma}\label{lem:mono}
    If $\mathcal{C}$ is a cluster in a $k$-dimensional subspace, then $\mathcal{C}$ must also form a cluster in any $(k-1)$-dimensional projection of the same space.
\end{lemma}
A proof is provided in \cite{clique}.

\subsection{Contributions}
The primary focus of this paper is to analyze the grid-based bottom-up subspace clustering algorithm \textit{MAFIA} \cite{mafia}, which builds upon the first grid-based subspace clustering approach \textit{CLIQUE} \cite{clique}. The relationship between these two is examined. Additionally, the density-connected algorithm \textit{SUBCLU} \cite{subclu} is included, as it offers a contrasting approach to the grid-based approach. All three algorithms are also evaluated experimentally in terms of scalability and clustering quality.

The remainder of the paper is structured as follows: Section 2 describes and analyzes the three algorithms in detail. Section 3 evaluates their performance in terms of scalability, considering dataset size, data- and cluster-dimensionality, as well as their clustering quality. Section 4 discusses the findings and the methods used. Finally, Section 6 concludes the main findings and suggests future work.